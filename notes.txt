SFNO Paper uses InstanceNorm, ai-models uses LayerNorm
InstanceNorm (Paper): same operation globally, making it equally equivariant
LayerNorm    (nirgends) : all the hidden units in a layer share the same normalization terms μ and σ
'''                     -> Constant summed input to next layer (useful for ReLU)

Model Architecture:
- outer_skip = None
- inner_skip = None
- big_skip = True
- mlp_mode = distributed
- norm_layer = InstanceNorm2d
- drop_rate = 0.0
- filter_type = non-linear
- spectral_layers = 3
- rank = 128
- use_complex_kernels = True
- compression = None
- sparsity_threshold = 0.0
- mlp_ratio = 2.0
- num_blocks = 8 -- irrelevant
- num_layers = 12

Network:
 1. residual
 2. encoder
 3. pos_drop
 4. # blocks by num_layers
 5. cat (x,residual)
 6. decoder

Block:
 1. norm
 2. filter
 3. GELU
 4. mlp

Filter:
 1. # spectral_layers
 2. W*x
 3. complex_relu


 check Architecture of original sfno on your slides for number of blocks
 no inner skips

 afine tranformation on complex numbers